# -*- coding: utf-8 -*-
"""
livelink_voice_chat.py â€” Unity ì—°ë™ Voiceâ€‘toâ€‘Voice GPT Demo
----------------------------------------------------------------
â€¢ [ìµœì¢… ìˆ˜ì •] ë¶ˆì•ˆì •í•œ TCP ì†Œì¼“ í†µì‹ ì„ ì œê±°í•˜ê³ , ì•ˆì •ì ì¸ ë‹¨ì¼ HTTP í†µì‹  ë°©ì‹ìœ¼ë¡œ ë³€ê²½
â€¢ ìŒì„± ìš”ì²­(POST)ì— ëŒ€í•œ HTTP ì‘ë‹µìœ¼ë¡œ ì§ì ‘ JSON(ê°ì •, GPT ë‹µë³€)ì„ ë°˜í™˜
â€¢ OpenAI SDK â‰¥ 1.30 í˜¸í™˜ (audio ë„¤ì„ìŠ¤í˜ì´ìŠ¤)
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° í‘œì¤€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os, sys, tempfile, warnings, wave
from pathlib import Path
from threading import Thread, Event
import httpx
import numpy as np
import soundfile as sf
import openai
import pygame
from flask import Flask, request, jsonify
from datetime import datetime
from g2pk import G2p
from utils.voice_conversion.voice_conversion import run_voice_conversion
from utils.romanize_ko import romanize_korean
from utils.audio_face_workers import process_wav_file
from livelink.connect.livelink_init import initialize_py_face, create_socket_connection, create_tcp_connection
from livelink.animations.default_animation import default_animation_loop
from utils.emotion_recognition.predict_emotion import predict_emotion
import socket
import time
import torch
from models.LLM import LLM
from models.TTS import TTS
from models.STT import STT


# LLM (Large Language Model) ì„¤ì •
llm_config = {'disable_chat_history': False,'model': 'llama3.1-8b-instruct-q4_0'}
# STT (Speech-to-Text) ì„¤ì •
stt_config = {'device': 'cuda','generation_args': {'batch_size': 8},'model': 'openai/whisper-small'}
# TTS (Text-to-Speech) ì„¤ì •
tts_config = {'device': 'cuda', 'model': 'tts_models/multilingual/multi-dataset/xtts_v2'}



stt_model = STT(**stt_config) if stt_config else None
tts_model = TTS(**tts_config) if tts_config else None
llm_model = LLM(**llm_config)

if not llm_model.exists():
    print(f"Invalid ollama model")
    exit()
    
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¤ì •ê°’ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TTS_MODEL = "tts-1-hd"
TRANSCRIBE_MODEL = "whisper-1"
GPT_MODEL = "gpt-4o-mini"
AUDIO_SAMPLE_RATE = 16_000
OUTPUT_WAV_DIR = Path.cwd() / "wav_cache"
OUTPUT_WAV_DIR.mkdir(parents=True, exist_ok=True)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ OpenAI í´ë¼ì´ì–¸íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    client = openai.OpenAI(
        api_key=os.getenv("OPENAI_API_KEY"),
        organization=os.getenv("OPENAI_ORGANIZATION"),
    )
except openai.OpenAIError as e:
    print(f"Error initializing OpenAI client: {e}")
    sys.exit(1)

warnings.filterwarnings("ignore", message="Couldn't find ffmpeg or avconv")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Voice preset discovery â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_DIR = Path(__file__).resolve().parent
VOICE_PRETRAINED_DIR = BASE_DIR / "utils" / "voice_conversion" / "pretrained"
AVAILABLE_VOICES = ["MaleYoung", "MaleOld", "FemaleYoung", "FemaleOld", "basic"]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì „ì—­ ë³€ìˆ˜ ë° Flask ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€
current_voice_name = AVAILABLE_VOICES[0]
app = Flask(__name__)

history = [{"role": "system", "content": "You are a helpful assistant. ë‹µë³€ì€ **í•œê¸€**ë¡œ, ìµœëŒ€ 100ìë¡œ ì œí•œí•´."}]

# [ì‚­ì œ] TCP ì†Œì¼“ ê´€ë ¨ ì „ì—­ ë³€ìˆ˜ ì œê±°
# py_face, socket_conn, anim_th ë“± 3D ì–¼êµ´ ê´€ë ¨ ë³€ìˆ˜ëŠ” ìœ ì§€í•©ë‹ˆë‹¤.
py_face = None
socket_conn = None
anim_th = None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Unity ì—°ë™ API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route('/set_voice/<voice_name>')
def set_voice(voice_name):
    global current_voice_name
    for available_voice in AVAILABLE_VOICES:
        if voice_name.lower() == available_voice.lower():
            current_voice_name = available_voice
            print(f"âœ… Voice changed to: {current_voice_name}")
            return f"Successfully set voice to {current_voice_name}"
    print(f"Input is Basic or Unknown voice: {voice_name}")
    current_voice_name = "Basic"
    return f"Set voice to Basic model: {current_voice_name}"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ [ì‚­ì œ] TCP ì„œë²„ ê´€ë ¨ í•¨ìˆ˜ ì „ì²´ ì œê±° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def run_tcp_server(): ...
# def info_sent_to_unity(...): ...



# llama ëª¨ë¸ ì ìš©
def llama_response(prompt: str, history: list[dict]) -> str:
    """GPT ëª¨ë¸ì— í”„ë¡¬í”„íŠ¸ë¥¼ ë³´ë‚´ê³  ì‘ë‹µì„ ë°›ìŒ"""
    answer = llm_model.forward(prompt)
    return answer



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì˜¤ë””ì˜¤ ìœ í‹¸ë¦¬í‹° ë° V2V íŒŒì´í”„ë¼ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def transcribe_audio(wav_path: Path) -> str:
    with wav_path.open("rb") as f:
        return client.audio.transcriptions.create(model=TRANSCRIBE_MODEL, file=f, response_format="text").strip()

def gpt_response(prompt: str, history: list[dict]) -> str:
    history.append({"role": "user", "content": prompt})
    resp = client.chat.completions.create(model=GPT_MODEL, messages=history, temperature=0.7)
    answer = resp.choices[0].message.content.strip()
    history.append({"role": "assistant", "content": answer})
    return answer

TTS_VOICE = "nova"
def text_to_speech(text: str, speed: float = 1.0) -> Path:
    resp = client.audio.speech.create(
        model=TTS_MODEL, voice=TTS_VOICE, input=text,
        response_format="wav", speed=speed, timeout=httpx.Timeout(20.0, connect=5.0)
    )
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
        path = f.name
    resp.stream_to_file(path)
    return Path(path)

def text_to_speech_llama(text: str) -> Path:
    if not tts_model:
        raise RuntimeError("TTS ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    print(f'ğŸ—£ï¸ ë¡œì»¬ TTS ëª¨ë¸ í˜¸ì¶œ: "{text}"')

    # 1) í•©ì„±
    audio = tts_model.forward(text=text, output_filepath="")  # forwardê°€ íŒŒì¼ì„ ì“°ì§€ ì•ŠëŠ”ë‹¤ë©´ ê·¸ëŒ€ë¡œ

    # 2) numpy 1D float32 [-1,1]ë¡œ ì •ë¦¬
    if isinstance(audio, torch.Tensor):
        audio = audio.detach().cpu().numpy()
    audio = np.asarray(audio, dtype=np.float32)
    if audio.ndim > 1:
        audio = audio.squeeze()
    # ì•ˆì „ í´ë¦¬í•‘
    audio = np.clip(audio, -1.0, 1.0)

    # 3) ìƒ˜í”Œë ˆì´íŠ¸: ëª¨ë¸ì—ì„œ ì–»ê¸°
    # Coqui TTS(api) ì¸ìŠ¤í„´ìŠ¤ì— ë³´í†µ ì•„ë˜ ì¤‘ í•˜ë‚˜ê°€ ìˆìŠµë‹ˆë‹¤.
    sr = getattr(tts_model.model, "output_sample_rate", None) \
         or getattr(getattr(tts_model.model, "synthesizer", None), "output_sample_rate", None) \
         or 24000  # ìµœí›„ì˜ ìˆ˜ë‹¨

    # 4) íŒŒì¼ë¡œ ì €ì¥ (PCM16)
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
        out_path = Path(f.name)

    sf.write(out_path.as_posix(), audio, sr, subtype="PCM_16")

    print(f"ğŸµ ì˜¤ë””ì˜¤ê°€ '{out_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (sr={sr}, len={len(audio)/sr:.2f}s)")
    return out_path



def numpy_to_wav_in_cache(audio_np: np.ndarray, sr: int) -> Path:
    timestamp = int(time.time() * 1000)
    file_name = f"rvc_{timestamp}.wav"
    path = OUTPUT_WAV_DIR / file_name
    sf.write(path, audio_np, sr)
    return path

def _rvc_pipeline(tts_path: Path, voice_name: str) -> Path:
    wav_np, sr = sf.read(tts_path, dtype="float32")
    converted_np, converted_sr = run_voice_conversion(
        src_audio=wav_np, src_sr=sr, transpose=0, voice_name=voice_name
    )
    return numpy_to_wav_in_cache(converted_np, converted_sr)

# [ìˆ˜ì •] build_voice_reply_audio í•¨ìˆ˜ê°€ (ì˜¤ë””ì˜¤ ê²½ë¡œ, JSON ë°ì´í„°) íŠœí”Œì„ ë°˜í™˜í•˜ë„ë¡ ë³€ê²½
def build_voice_reply_audio(wav_path: Path, history: list[dict], voice_name: str) -> tuple[Path, dict]:
    print("1. Transcribing user audio...")
    user_text = transcribe_audio(wav_path)
    print(f"   > User said: {user_text}")

    print("1.5 Predict emotion...")
    emotion_result, emotion_probability = predict_emotion(wav_path)
    print(f"   > emotion_results: {emotion_result, emotion_probability}")

    #print("2. Getting response from GPT...")
    #assistant_ko = gpt_response(user_text, history)
    
    print("2. Getting response from Llama...")
    assistant_ko = llama_response(user_text, history)
    
    
    assistant_en = romanize_korean(assistant_ko)
    
    print(f"   > GPT responds: {assistant_ko}")
    
    # [ì‚­ì œ] TCPë¡œ ë°ì´í„°ë¥¼ ë³´ë‚´ëŠ” í•¨ìˆ˜ í˜¸ì¶œ ì œê±°
    # info_sent_to_unity(...)

    # [ì¶”ê°€] Unity í´ë¼ì´ì–¸íŠ¸ë¡œ ë°˜í™˜í•  JSON ë°ì´í„°ë¥¼ ìƒì„±
    response_data = {
        "emotion": emotion_result,
        "probability": emotion_probability,
        "response": assistant_ko
    }
    
    print(f"3. Converting text to speech with '{voice_name}' voice...")
    #tts_path = text_to_speech(assistant_en, speed=0.9)
    tts_path = text_to_speech_llama(assistant_ko)
    
    
    final_audio_path = tts_path
    #if voice_name != "Basic":
        #final_audio_path = _rvc_pipeline(tts_path, voice_name)
    
    # ìƒì„±ëœ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œì™€ í•¨ê»˜ response_data ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜
    return final_audio_path, response_data

# [ìˆ˜ì •] voice_input ì—”ë“œí¬ì¸íŠ¸ê°€ ì§ì ‘ JSONì„ ë°˜í™˜í•˜ë„ë¡ ë³€ê²½
@app.route('/voice_input', methods=['POST'])
def handle_voice_input():
    
    print(f"\n{'ğŸ¤'*25} VOICE INPUT ìš”ì²­ ì²˜ë¦¬ ì‹œì‘ {'ğŸ¤'*25}")
    
    client_ip = request.remote_addr
    
    if not request.data:
        print("âŒ ì—ëŸ¬: ìš”ì²­ì— ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return jsonify({"error": "No audio data in request"}), 400

    print(f"ğŸ“¦ ë°ì´í„° ìˆ˜ì‹  ì™„ë£Œ: {len(request.data) / 1024:.1f} KB")

    # 1. ìˆ˜ì‹ ëœ ì˜¤ë””ì˜¤ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    try:
        timestamp = int(time.time())
        temp_wav_path = OUTPUT_WAV_DIR / f"received_{timestamp}.wav"
        with open(temp_wav_path, 'wb') as f:
            f.write(request.data)
        print(f"ğŸ’¾ ìˆ˜ì‹ ëœ ì˜¤ë””ì˜¤ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥: {temp_wav_path}")
    except Exception as e:
        print(f"âŒ ì„ì‹œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        return jsonify({"error": f"Failed to save temporary audio file: {e}"}), 500

    # 2. í•µì‹¬! ìŒì„± ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    try:
        global history, current_voice_name
        
        # build_voice_reply_audio í•¨ìˆ˜ëŠ” ì´ì œ (ì˜¤ë””ì˜¤ ê²½ë¡œ, JSON ë°ì´í„°)ë¥¼ ë°˜í™˜
        final_audio_path, response_json = build_voice_reply_audio(Path(temp_wav_path), history, current_voice_name)
        
        process_wav_file(final_audio_path, py_face, socket_conn, anim_th, client_ip, stop_default_animation_flag)
        
        print(f"âœ… ìŒì„± ì‘ë‹µ ìƒì„± ì™„ë£Œ: {final_audio_path}")
        print(f"ğŸ“¤ Unityë¡œ ì‘ë‹µí•  JSON ë°ì´í„°: {response_json}")
        
        # [í•µì‹¬ ìˆ˜ì •] TCPë¡œ ë³´ë‚´ëŠ” ëŒ€ì‹ , HTTP ì‘ë‹µìœ¼ë¡œ JSON ë°ì´í„°ë¥¼ ì§ì ‘ ë°˜í™˜í•©ë‹ˆë‹¤.
        return jsonify(response_json), 200

    except openai.APIError as e:
        print(f"âŒ OpenAI API ì—ëŸ¬: {e}")
        return jsonify({"error": f"OpenAI API Error: {e}"}), 500
    except Exception as e:
        import traceback
        print(f"âŒ ìŒì„± ì²˜ë¦¬ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì—ëŸ¬ ë°œìƒ: {e}")
        traceback.print_exc()
        return jsonify({"error": f"An unexpected error occurred during voice processing: {e}"}), 500
    finally:
        print(f"{'ğŸ¤'*25} VOICE INPUT ìš”ì²­ ì²˜ë¦¬ ì™„ë£Œ {'ğŸ¤'*25}\n")

@app.route('/')
def index():
    return "ì„œë¹„ìŠ¤ê°€ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤"

stop_default_animation_flag = Event() 

def manage_unity_connection_and_animation(host='127.0.0.1', port=5002):
    """
    ë³„ë„ì˜ ìŠ¤ë ˆë“œì—ì„œ Unity í´ë¼ì´ì–¸íŠ¸ì˜ ì—°ê²°ì„ ê¸°ë‹¤ë¦¬ê³ ,
    ì—°ê²° ì„±ê³µ ì‹œ ì• ë‹ˆë©”ì´ì…˜ ë£¨í”„ë¥¼ ì‹œì‘í•˜ëŠ” í•¨ìˆ˜.
    """
    # ì´ í•¨ìˆ˜ì—ì„œ ìˆ˜ì •í•  ì „ì—­ ë³€ìˆ˜ë“¤ì„ ì„ ì–¸í•©ë‹ˆë‹¤.
    global socket_conn, anim_th

    # TCP ì„œë²„ ì†Œì¼“ ì„¤ì •
    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    
    try:
        server_socket.bind((host, port))
        server_socket.listen(1)
        print(f"ğŸ”Œ (ìŠ¤ë ˆë“œ) Unity ì—°ê²° ëŒ€ê¸° ì¤‘... (IP: {host}, Port: {port})")

        # ğŸ‘‡ ì—¬ê¸°ê°€ í•µì‹¬! ì´ accept()ëŠ” ì´ì œ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ ë©”ì¸ í”„ë¡œê·¸ë¨ì„ ë§‰ì§€ ì•ŠìŠµë‹ˆë‹¤.
        client_socket, addr = server_socket.accept()
        
        # ì—°ê²° ì„±ê³µ ì‹œ ì „ì—­ ë³€ìˆ˜ì— ì†Œì¼“ í• ë‹¹
        socket_conn = client_socket
        print(f"âœ… Unity í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì„±ê³µ! (from: {addr})")

        # 3D ì–¼êµ´ ê°ì²´(py_face)ê°€ ì´ˆê¸°í™”ë˜ì—ˆë‹¤ë©´ ì• ë‹ˆë©”ì´ì…˜ ìŠ¤ë ˆë“œ ì‹œì‘
        if py_face:
            anim_th = Thread(target=default_animation_loop, args=(py_face, socket_conn, stop_default_animation_flag), daemon=True)
            anim_th.start()
            print("âœ… 3D ì–¼êµ´ ê¸°ë³¸ ì• ë‹ˆë©”ì´ì…˜ ìŠ¤ë ˆë“œ ì‹œì‘!")

    except Exception as e:
        print(f"âŒ (ìŠ¤ë ˆë“œ) Unity ì—°ê²° ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        print("âš ï¸ Unityì™€ ì—°ê²°ë˜ì§€ ì•Šì•„ 3D ì–¼êµ´ ì• ë‹ˆë©”ì´ì…˜ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
        socket_conn = None # ì‹¤íŒ¨ ì‹œ Noneìœ¼ë¡œ ìœ ì§€
    finally:
        # ì—°ê²° ëŒ€ê¸° ì†Œì¼“ì€ ë‹«ì•„ì¤ë‹ˆë‹¤.
        server_socket.close()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main() â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    global py_face, socket_conn, anim_th

    # IP ì£¼ì†Œ ì •ë³´ ì¶œë ¥
    hostname = socket.gethostname()
    local_ip = socket.gethostbyname(hostname)
    
    print("="*60)
    print("ğŸŒ ë„¤íŠ¸ì›Œí¬ ì •ë³´:")
    print(f"   ğŸ“ í˜¸ìŠ¤íŠ¸ëª…: {hostname}")
    print(f"   ğŸ“ ë¡œì»¬ IP: {local_ip}")
    print(f"   ğŸŒ ì ‘ì† URL: http://{local_ip}:5001")
    print("="*60)

    # [ì‚­ì œ] TCP ì„œë²„ ìŠ¤ë ˆë“œ ì‹œì‘ ì½”ë“œ ì œê±°
    # print("ğŸš€ TCP ì„œë²„ ì‹œì‘ ì¤‘...")
    # tcp_thread = ...
    # tcp_thread.start()
    
    # 3D ì–¼êµ´ ë° í†µì‹  ì´ˆê¸°í™” (ì„ íƒì  ê¸°ëŠ¥)
    try:
        '''print("ğŸ­ 3D ì–¼êµ´ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        py_face = initialize_py_face()
        socket_conn = create_socket_connection()
        stop_default_animation_flag =  Event()
        anim_th = Thread(target=default_animation_loop, args=(py_face, stop_default_animation_flag), daemon=True)
        anim_th.start()
        print("âœ… 3D ì–¼êµ´ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")'''
        
        print("ğŸ­ 3D ì–¼êµ´ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        py_face = initialize_py_face()

        # ğŸ‘‡ ì—¬ê¸°ë¥¼ ìˆ˜ì •!
        #socket_conn = create_tcp_connection() # UDP ëŒ€ì‹  TCP ì—°ê²° í•¨ìˆ˜ í˜¸ì¶œ
        connection_thread = Thread(target=manage_unity_connection_and_animation, daemon=True)
        connection_thread.start()



        # ğŸ‘‡ ì—°ê²° ì‹¤íŒ¨ ì‹œ ì²˜ë¦¬
        """if socket_conn is None:
            print("âš ï¸ Unityì™€ ì—°ê²°ë˜ì§€ ì•Šì•„ 3D ì–¼êµ´ ì• ë‹ˆë©”ì´ì…˜ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
            py_face = None
            # í”„ë¡œê·¸ë¨ì´ ê³„ì† ì‹¤í–‰ë˜ì–´ì•¼ í•œë‹¤ë©´ ì•„ë˜ ì½”ë“œëŠ” ìœ ì§€
        else:
            stop_default_animation_flag = Event()
            
            anim_th = Thread(target=default_animation_loop, args=(py_face, socket_conn, stop_default_animation_flag), daemon=True)
            anim_th.start()
            print("âœ… 3D ì–¼êµ´ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")"""
        
        
        
    except Exception as e:
        print(f"âš ï¸ 3D ì–¼êµ´ ì´ˆê¸°í™” ì‹¤íŒ¨ (Flask ì„œë²„ëŠ” ê³„ì† ì§„í–‰): {e}")
        py_face = socket_conn = anivm_th = None
    
    print("="*50)
    print(f"ğŸš€ Flask ì„œë²„ ì‹œì‘ ì¤‘...")
    print(f"ğŸ“ ë¡œì»¬ ì ‘ì†: http://localhost:5001")
    print(f"ğŸ“ ë„¤íŠ¸ì›Œí¬ ì ‘ì†: http://{local_ip}:5001")
    print(f"ğŸ“ API ì—”ë“œí¬ì¸íŠ¸: http://{local_ip}:5001/voice_input")
    print(f"ğŸ¤ ê¸°ë³¸ ìŒì„±: {current_voice_name}")
    print("="*50)

    try:
        app.run(
            host='0.0.0.0', 
            port=5001, 
            debug=True,
            use_reloader=False,
            threaded=True
        )
    except Exception as e:
        print(f"âŒ Flask ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì„œë²„ ì¢…ë£Œ ì¤‘...")
    finally:
        print("\nâ¹ï¸ ì„œë²„ ì¢…ë£Œ ì¤‘...")
        if 'stop_default_animation' in globals():
            stop_default_animation_flag.set()
        if anim_th and anim_th.is_alive():
            anim_th.join(timeout=2)
        if socket_conn:
            socket_conn.close()
        print("âœ… ì„œë²„ ì¢…ë£Œ ì™„ë£Œ")


if __name__ == "__main__":
    main()