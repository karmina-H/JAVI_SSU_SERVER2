from __future__ import annotations
"""
livelink_voice_chat.py â€” Voiceâ€‘toâ€‘Voice GPT Demo (í™•ì¥íŒ)
-------------------------------------------------------
â€¢ ë§ˆì´í¬ ì…ë ¥, wav íŒŒì¼, **í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥** 3â€‘way ì¸í„°í˜ì´ìŠ¤
â€¢ í•œêµ­ì–´ ë‹µë³€ â†’ ë¡œë§ˆì ë³€í™˜(g2pk) â†’ ì˜ì–´ TTS â†’ NeuroSync ìŠ¤íŠ¸ë¦¬ë°
â€¢ OpenAI SDK â‰¥â€¯1.30 í˜¸í™˜ (audio ë„¤ì„ìŠ¤í˜ì´ìŠ¤)

â€» 2025â€‘06â€‘14 ì—…ë°ì´íŠ¸
    - voice_name ì„ utils/voice_conversion/pretrained/ ì•„ë˜ í´ë” ì¤‘ì—ì„œ ì„ íƒí•˜ë„ë¡ ë³€ê²½.
    - TTS ê²°ê³¼ë¥¼ numpy ë¡œ ë³€í™˜ í›„ RVC ì²˜ë¦¬, ë‹¤ì‹œ wav ë¡œ ì €ì¥í•˜ë„ë¡ í†µì¼.
    - build_voice_reply_* í•¨ìˆ˜ ì¤‘ë³µ í˜¸ì¶œ ë° ê²½ë¡œ/ë°ì´í„° íƒ€ì… í˜¼ë™ ìˆ˜ì •.
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° í‘œì¤€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os, sys, queue, tempfile, warnings
from pathlib import Path
from threading import Thread
from contextlib import suppress
from types import GeneratorType
import httpx
import numpy as np
import soundfile as sf
<<<<<<< HEAD
=======
import openai           # OpenAI Python SDK (>=1.30)
>>>>>>> a6732d87576fed2a8e00dedfdf8f7b7a187b1bea
import pygame
import sounddevice as sd
from scipy.io.wavfile import write as wav_write
import json

import threading
import socket
<<<<<<< HEAD
import wave
=======
>>>>>>> a6732d87576fed2a8e00dedfdf8f7b7a187b1bea

from g2pk import G2p    # ë¡œë§ˆì ë³€í™˜
from utils.voice_conversion.voice_conversion import run_voice_conversion

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í”„ë¡œì íŠ¸ ë‚´ë¶€ ëª¨ë“ˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from utils.files.file_utils import initialize_directories, ensure_wav_input_folder_exists, list_wav_files
from utils.romanize_ko import romanize_korean
from utils.audio_face_workers import process_wav_file
from livelink.connect.livelink_init import initialize_py_face, create_socket_connection
from livelink.animations.default_animation import default_animation_loop, stop_default_animation
from utils.emote_sender.send_emote import EmoteConnect
from utils.emotion_recognition.predict_emotion  import predict_emotion

os.environ["OPENAI_API_KEY"] = ""

from flask import Flask

<<<<<<< HEAD
############# stt,tts,llama init  ####################
from models.LLM import LLM
from models.TTS import TTS
from models.STT import STT

# LLM (Large Language Model) ì„¤ì •
llm_config = {'disable_chat_history': False,'model': 'llama3.1-8b-instruct-q4_0'}
# STT (Speech-to-Text) ì„¤ì •
stt_config = {'device': 'cuda','generation_args': {'batch_size': 8},'model': 'openai/whisper-small'}
# TTS (Text-to-Speech) ì„¤ì •
tts_config = {'device': 'cuda', 'model': 'tts_models/multilingual/multi-dataset/xtts_v2'}



stt_model = STT(**stt_config) if stt_config else None
tts_model = TTS(**tts_config) if tts_config else None
llm_model = LLM(**llm_config)

# if not stt_model.exists():
#     print(f"Invalid stt_model model")
#     exit()
# if not tts_model.exists():
#     print(f"Invalid tts_model model")
#     exit()
if not llm_model.exists():
    print(f"Invalid ollama model")
    exit()

##################

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¤ì •ê°’ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
=======
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¤ì •ê°’ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TTS_MODEL = "tts-1"
TRANSCRIBE_MODEL = "whisper-1"
GPT_MODEL = "gpt-4o-mini"
>>>>>>> a6732d87576fed2a8e00dedfdf8f7b7a187b1bea
AUDIO_SAMPLE_RATE = 16_000
OUTPUT_WAV_DIR = Path.cwd() / "wav_cache"
OUTPUT_WAV_DIR.mkdir(parents=True, exist_ok=True)

<<<<<<< HEAD
=======
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ OpenAI í´ë¼ì´ì–¸íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    client = openai.OpenAI(
        api_key=os.getenv("OPENAI_API_KEY"),
        organization=os.getenv("OPENAI_ORGANIZATION"),
    )
except openai.OpenAIError as e:
    print(f"Error initializing OpenAI client: {e}")
    sys.exit(1)

warnings.filterwarnings("ignore", message="Couldn't find ffmpeg or avconv")

>>>>>>> a6732d87576fed2a8e00dedfdf8f7b7a187b1bea
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Voice preset discovery â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ ìˆëŠ” í´ë”ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ ì„¤ì •
BASE_DIR = Path(__file__).resolve().parent
VOICE_PRETRAINED_DIR = BASE_DIR / "utils" / "voice_conversion" / "pretrained"

# (ì´ ë¶€ë¶„ì€ ì‹¤ì œ RVC ëª¨ë¸ì„ ì‚¬ìš©í•  ë•Œ í•„ìš”í•˜ë¯€ë¡œ, ê²½ë¡œê°€ ì—†ì–´ë„ ì¼ë‹¨ ì‹¤í–‰ë˜ë„ë¡ ìˆ˜ì •)
AVAILABLE_VOICES = []
if VOICE_PRETRAINED_DIR.exists() and VOICE_PRETRAINED_DIR.is_dir():
    AVAILABLE_VOICES = [d.name for d in VOICE_PRETRAINED_DIR.iterdir() if d.is_dir()]

if not AVAILABLE_VOICES:
    print("âš ï¸  Warning: No local RVC voice presets found. Using default OpenAI voices.")
    # ë¡œì»¬ ë³´ì´ìŠ¤ê°€ ì—†ì„ ê²½ìš°, OpenAI ê¸°ë³¸ ë³´ì´ìŠ¤ë¡œ ëŒ€ì²´
    AVAILABLE_VOICES = ["nova", "IU", "KARINA", "ENIME", "Puth"]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìœ ë‹ˆí‹° ì—°ë™ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ ë° Flask ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€
current_voice_name = AVAILABLE_VOICES[0]  # ê¸°ë³¸ê°’ìœ¼ë¡œ ì²« ë²ˆì§¸ ëª©ì†Œë¦¬ ì„¤ì •
app = Flask(__name__)

def run_flask_server():
    app.run(host='127.0.0.1', port=5001, debug=False)

@app.route('/set_voice/<voice_name>')
def set_voice(voice_name):
    global current_voice_name
    # ë¡œì»¬ ë³´ì´ìŠ¤ ëª©ë¡ ë˜ëŠ” OpenAI ê¸°ë³¸ ë³´ì´ìŠ¤ ëª©ë¡ì— ìˆëŠ”ì§€ í™•ì¸
    if voice_name in AVAILABLE_VOICES:
        current_voice_name = voice_name
        print(f"âœ… Voice changed to: {current_voice_name}")
        return f"Successfully set voice to {current_voice_name}"
    else:
        # ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  ë¹„êµ
        for available_voice in AVAILABLE_VOICES:
            if voice_name.lower() == available_voice.lower():
                current_voice_name = available_voice
                print(f"âœ… Voice changed to: {current_voice_name}")
                return f"Successfully set voice to {current_voice_name}"
        
        print(f"Input is Basic or  Unknown voice: {voice_name}")
        return f"Set voice to Basic model : {current_voice_name}"

# [ì¶”ê°€] TCP ì†Œì¼“ í†µì‹ ì„ ìœ„í•œ ì„¤ì •
TCP_HOST = '127.0.0.1'
TCP_PORT = 9999
unity_conn = None # ìœ ë‹ˆí‹°ì™€ì˜ TCP ì—°ê²°ì„ ì €ì¥í•  ì „ì—­ ë³€ìˆ˜

# [ì¶”ê°€] Unityì˜ ë°ì´í„° ìˆ˜ì‹  ì—°ê²°ì„ ì²˜ë¦¬í•  TCP ì„œë²„ í•¨ìˆ˜
def run_tcp_server():
    """Unity í´ë¼ì´ì–¸íŠ¸ì˜ ì—°ê²°ì„ ìˆ˜ë½í•˜ê³  ì „ì—­ ë³€ìˆ˜ì— ì €ì¥í•˜ëŠ” ì„œë²„"""
    global unity_conn
    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    server_socket.bind((TCP_HOST, TCP_PORT))
    server_socket.listen()
    print(f"âœ… TCP Server for Unity is running on {TCP_HOST}:{TCP_PORT}")
    print("Waiting for Unity data client to connect...")
    unity_conn, _ = server_socket.accept()
    print("ğŸ”— Unity data client connected!")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Emotion&LLM Response To Unity í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def info_sent_to_unity(emotion: str, probability: float, response: str):
    global unity_conn
    """emotion, probability, response ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ë¬¶ì–´ Unityì— ì „ì†¡í•©ë‹ˆë‹¤."""
    if unity_conn is None:
        print("âŒ Unityê°€ TCP ì†Œì¼“ì— ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    try:
        # [ìˆ˜ì •] data_to_send ë”•ì…”ë„ˆë¦¬ì— 'probability' í•­ëª© ì¶”ê°€
        data_to_send = {
            "emotion": emotion,
            "probability": probability,
            "response": response
        }
        
        json_string = json.dumps(data_to_send) + "\n"
        unity_conn.sendall(json_string.encode('utf-8'))
        print(f"ì „ì†¡ ì™„ë£Œ: {json_string.strip()}")

    except (ConnectionResetError, BrokenPipeError):
        print("âŒ Unity TCP ì—°ê²°ì´ ëŠì–´ì¡ŒìŠµë‹ˆë‹¤.")
        unity_conn = None
    except Exception as e:
        print(f"ì „ì†¡ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì˜¤ë””ì˜¤ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def record_microphone() -> Path:
    """ì—”í„° â†’ ë…¹ìŒ ì‹œì‘, ì—”í„° â†’ ì¢…ë£Œ í›„ temp wav íŒŒì¼ ë°˜í™˜"""
    q: queue.Queue[np.ndarray] = queue.Queue()

    def _callback(indata, _frames, _time, status):
        if status:
            print(status, file=sys.stderr)
        q.put(indata.copy())

    print("\nğŸ™ï¸  Press Enter to start recording, and Enter again to stop.")
    input()  # ì²« ë²ˆì§¸ Enter ëŒ€ê¸°
    print("Recordingâ€¦")
    
    stream = sd.InputStream(samplerate=AUDIO_SAMPLE_RATE, channels=1, dtype="int16", callback=_callback)
    with stream:
        input() # ë‘ ë²ˆì§¸ Enter ëŒ€ê¸°
    
    print("Recording stopped. Processingâ€¦")

    if q.empty():
        raise RuntimeError("No audio captured.")
    
    audio_np = np.concatenate([q.get() for _ in range(q.qsize())], axis=0)
    
    # ì„ì‹œ íŒŒì¼ ìƒì„±
    fd, path = tempfile.mkstemp(suffix=".wav")
    os.close(fd)
    
<<<<<<< HEAD
    print(f"ë…¹ìŒëœê±° path: {path}")
    wav_write(path, AUDIO_SAMPLE_RATE, audio_np)
    return Path(path)


def transcribe_audio(wav_path: Path) -> str:
    """ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œë¥¼ ë°›ì•„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    audio_data = None
    with wave.open(str(wav_path), 'rb') as wf:
        # ì˜¤ë””ì˜¤ íŒŒì¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        n_channels = wf.getnchannels()
        sampwidth = wf.getsampwidth()
        framerate = wf.getframerate()
        n_frames = wf.getnframes()
        
        # í”„ë ˆì„ ì½ê¸°
        frames = wf.readframes(n_frames)
        audio_data = np.frombuffer(frames, dtype=np.int16)
        
    if audio_data is not None:
        # stt_model.forwardì˜ ì…ë ¥ì€ ì¼ë°˜ì ìœ¼ë¡œ NumPy ë°°ì—´ ë˜ëŠ” í…ì„œ í˜•íƒœì…ë‹ˆë‹¤.
        # `openai/whisper-small` ëª¨ë¸ì˜ ê²½ìš°, ì¼ë°˜ì ìœ¼ë¡œ 16kHz ëª¨ë…¸ ì˜¤ë””ì˜¤ë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤.
        # `audio_data` ë³€ìˆ˜ê°€ ì´ ëª¨ë¸ì˜ ì…ë ¥ í˜•ì‹ì— ë§ì•„ì•¼ í•©ë‹ˆë‹¤.
        transcription = stt_model.forward(audio_data)
        print("ë³€í™˜ì„±ê³µ")
        return transcription
    else:
        print("ë³€í™˜ì‹¤íŒ¨")
        return None
    

def llama_response(prompt: str, history: list[dict]) -> str:
    """GPT ëª¨ë¸ì— í”„ë¡¬í”„íŠ¸ë¥¼ ë³´ë‚´ê³  ì‘ë‹µì„ ë°›ìŒ"""
    answer = llm_model.forward(prompt)
    return answer

def text_to_speech(text: str,speed: float = 1.0) -> Path:
    print("ttsí•¨ìˆ˜ì‹œì‘")
    output_file_path = "output.wav"
    synthesis = tts_model.forward(text, output_file_path)
    tts_model.model.synthesizer.save_wav(wav=synthesis, path=output_file_path)
    return Path(output_file_path)
=======
    wav_write(path, AUDIO_SAMPLE_RATE, audio_np)
    return Path(path)

def transcribe_audio(wav_path: Path) -> str:
    """ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œë¥¼ ë°›ì•„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    with wav_path.open("rb") as f:
        return client.audio.transcriptions.create(model=TRANSCRIBE_MODEL, file=f, response_format="text").strip()

def gpt_response(prompt: str, history: list[dict]) -> str:
    """GPT ëª¨ë¸ì— í”„ë¡¬í”„íŠ¸ë¥¼ ë³´ë‚´ê³  ì‘ë‹µì„ ë°›ìŒ"""
    history.append({"role": "user", "content": prompt})
    resp = client.chat.completions.create(model=GPT_MODEL, messages=history, temperature=0.7)
    answer = resp.choices[0].message.content.strip()
    history.append({"role": "assistant", "content": answer})
    return answer

TTS_VOICE   = "nova" 

def text_to_speech(text: str,speed: float = 1.0) -> Path:
    """í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì„ì‹œ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
    resp = client.audio.speech.create(
        model=TTS_MODEL,
        voice=TTS_VOICE,
        input=text,
        response_format="wav",
        speed=speed,
        timeout=httpx.Timeout(20.0, connect=5.0) 
    )
    fd, path = tempfile.mkstemp(suffix=".wav")
    os.close(fd)
    resp.stream_to_file(path)
    return Path(path)
>>>>>>> a6732d87576fed2a8e00dedfdf8f7b7a187b1bea

def play_audio(file_path: Path):
    """ì£¼ì–´ì§„ ê²½ë¡œì˜ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì¬ìƒ"""
    try:
        data, fs = sf.read(file_path, dtype='float32')
        sd.play(data, fs)
        sd.wait()
    except Exception as e:
        print(f"Error playing audio: {e}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í—¬í¼: numpy â†’ ì§€ì • ê²½ë¡œ wav â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import time
def numpy_to_wav_in_cache(audio_np: np.ndarray, sr: int) -> Path:
    """wav_cache/rvc_<íƒ€ì„ìŠ¤íƒ¬í”„>.wav ë¡œ ì €ì¥ í›„ Path ë°˜í™˜"""
    
    # ê³ ìœ í•œ íŒŒì¼ ì´ë¦„ì„ ë§Œë“¤ê¸° ìœ„í•´ í˜„ì¬ ì‹œê°„ì„ ì´ìš©í•©ë‹ˆë‹¤.
    timestamp = int(time.time() * 1000)
    file_name = f"rvc_{timestamp}.wav" # ì˜ˆ: rvc_1686835200123.wav
    
    path = OUTPUT_WAV_DIR / file_name
    sf.write(path, audio_np, sr)
    return path


    


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ rvc Voice Conversion í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _rvc_pipeline(tts_path: Path, voice_name: str) -> Path:
    """TTS wav â†’ numpy â†’ RVC â†’ temp wav Path ë°˜í™˜"""
    wav_np, sr = sf.read(tts_path, dtype="float32")
    converted_np, converted_sr = run_voice_conversion(
        src_audio=wav_np,
        src_sr=sr,
        transpose=0,
        voice_name=voice_name,
    )
    return numpy_to_wav_in_cache(converted_np, converted_sr)



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ V2V íŒŒì´í”„ë¼ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_voice_reply_audio(wav_path: Path, history: list[dict], voice_name: str) -> Path:
    """ìŒì„± ì…ë ¥ â†’ STT â†’ GPT â†’ TTS (RVCëŠ” ë¹„í™œì„±í™”)"""
<<<<<<< HEAD
    #voice to text
=======
>>>>>>> a6732d87576fed2a8e00dedfdf8f7b7a187b1bea
    print("1. Transcribing user audio...")
    user_text = transcribe_audio(wav_path)
    print(f"   > User said: {user_text}")


    print("1.5 Predict emotion...")
    emotion_result, emotion_probability  = predict_emotion(wav_path)
    print(f"   > emotion_results: {emotion_result, emotion_probability}")

<<<<<<< HEAD
    #gpt
    print("2. Getting response from GPT...")
    assistant_ko = llama_response(user_text, history)
=======
    print("2. Getting response from GPT...")
    assistant_ko = gpt_response(user_text, history)
>>>>>>> a6732d87576fed2a8e00dedfdf8f7b7a187b1bea
    print(f"   > GPT responds: {assistant_ko}")
    
    print("2.5 Send Emotion and Response To Unity...")
    info_sent_to_unity(emotion_result, emotion_probability, assistant_ko)
    
    
    print(f"3. Converting text to speech with '{voice_name}' voice...")
    # RVCë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ìœ ë‹ˆí‹°ì—ì„œ ë°›ì€ voice_nameì„ ë°”ë¡œ TTSì— ì‚¬ìš©
    assistant_en = romanize_korean(assistant_ko)
    tts_path = text_to_speech(assistant_en, speed=0.9)
    
<<<<<<< HEAD
    

=======
>>>>>>> a6732d87576fed2a8e00dedfdf8f7b7a187b1bea
    if voice_name == "Basic":
        return tts_path
    else:
        # _rvc_pipeline ê¸°ëŠ¥ ì ì‹œ ì¤‘ì§€
        return _rvc_pipeline(tts_path, voice_name)
        
        #return tts_path
        


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main() â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    # 1. ì´ˆê¸°í™” (ê¸°ì¡´ ì½”ë“œì˜ í•„ìˆ˜ ì´ˆê¸°í™” ë¡œì§ í¬í•¨)
    initialize_directories()  # í•„ìš”í•˜ë‹¤ë©´ ì´ í•¨ìˆ˜ì˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”.
    wav_dir = Path.cwd() / "wav_input"
    ensure_wav_input_folder_exists(str(wav_dir)) # í•„ìš”í•˜ë‹¤ë©´ ì´ í•¨ìˆ˜ì˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”.

    ## ë‘ ê°œì˜ ì„œë²„ë¥¼ ê°ê° ë‹¤ë¥¸ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
    flask_thread = threading.Thread(target=run_flask_server, daemon=True)
    flask_thread.start()
    
    tcp_thread = threading.Thread(target=run_tcp_server, daemon=True)
    tcp_thread.start()
    
     # --- [ìˆ˜ì •/ì¶”ê°€] Unity TCP í´ë¼ì´ì–¸íŠ¸ê°€ ì—°ê²°ë  ë•Œê¹Œì§€ ë©”ì¸ ìŠ¤ë ˆë“œ ëŒ€ê¸° ---
    print("â³ Waiting for Unity TCP client to connect on port 9999...")
    while unity_conn is None:
        time.sleep(0.5) # 0.5ì´ˆ ê°„ê²©ìœ¼ë¡œ í™•ì¸ (íŒŒì¼ ìƒë‹¨ì— import time ì¶”ê°€ í•„ìš”)
    print("âœ… Unity TCP client is connected!")
    
    # 3D ì–¼êµ´ ë° í†µì‹  ì´ˆê¸°í™” (ê¸°ì¡´ ì½”ë“œì—ì„œ ê°€ì ¸ì˜´)
    py_face     = initialize_py_face()
    socket_conn = create_socket_connection()

    # ê¸°ë³¸ ì• ë‹ˆë©”ì´ì…˜ ìŠ¤ë ˆë“œ ì‹œì‘ (ê¸°ì¡´ ì½”ë“œì—ì„œ ê°€ì ¸ì˜´)
    anim_th = Thread(target=default_animation_loop, args=(py_face, stop_default_animation), daemon=True)
    anim_th.start()
    
    
    
    # 2. ì„œë²„ ì‹¤í–‰ ì•ˆë‚´ ë©”ì‹œì§€ ì¶œë ¥
    print("="*50)
    print("ğŸš€ Python Backend Server is running on http://127.0.0.1:5001")
    print(f"Default voice is set to: {current_voice_name}")
    print("Ready to receive voice change commands from Unity.")
    print("Starting microphone listening loop...")
    print("="*50)

    # 3. GPT ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
    history = [{
        "role": "system",
        "content": "You are a helpful assistant. ë‹µë³€ì€ **í•œê¸€**ë¡œ, ìµœëŒ€ 100ìë¡œ ì œí•œí•´.",
    }]

    # 4. ë©”ì¸ ë£¨í”„ ì‹¤í–‰
    while True:
        assistant_audio_path = None
        user_audio_path = None
        
        try:
            # ë§ˆì´í¬ë¡œ ë…¹ìŒ
            user_audio_path = record_microphone()
            
            # í˜„ì¬ ì„¤ì •ëœ ëª©ì†Œë¦¬(current_voice_name)ë¡œ ìŒì„± ì²˜ë¦¬
            assistant_audio_path = build_voice_reply_audio(user_audio_path, history, current_voice_name)
            
            # <<< ì¤‘ìš”! ì´ ë¶€ë¶„ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.
            # ìƒì„±ëœ ìŒì„± íŒŒì¼ì„ 3D ì–¼êµ´ë¡œ ë³´ë‚´ì„œ ì¬ìƒí•˜ê³  ì• ë‹ˆë©”ì´ì…˜ ì‹¤í–‰
            print(f"âœ… Generated response audio. Processing for 3D face...")
            print("asdasdasd", type(assistant_audio_path),type(py_face), type(socket_conn), type(anim_th))
            process_wav_file(assistant_audio_path, py_face, socket_conn, anim_th)
            
        except RuntimeError as e:
            print(f"Audio capture error: {e}")
        except KeyboardInterrupt:
            print("\nâ¹ï¸  Exiting microphone loop.")
            break
        except Exception as e:
            print(f"An unexpected error occurred: {e}")
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬ (ë³€ìˆ˜ëª… ìˆ˜ì •)
            if user_audio_path and user_audio_path.exists():
                with suppress(PermissionError): user_audio_path.unlink(missing_ok=True)
            if assistant_audio_path and assistant_audio_path.exists():
                with suppress(PermissionError): assistant_audio_path.unlink(missing_ok=True)
            
    # 5. ì¢…ë£Œ ì²˜ë¦¬ ë¡œì§ (ê¸°ì¡´ ì½”ë“œì—ì„œ ê°€ì ¸ì˜´)
    print("Shutting down...")
    stop_default_animation.set()
    anim_th.join(timeout=2)
    pygame.quit()
    socket_conn.close()
    
    
if __name__ == "__main__":
    main()
