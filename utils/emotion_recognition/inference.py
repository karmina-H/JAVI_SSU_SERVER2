import torch
import torchaudio
from transformers import Wav2Vec2Processor
from model import Wav2Vec2ForEmotion  # ì‚¬ìš©ì ì •ì˜ ëª¨ë¸ í´ë˜ìŠ¤
import os
from pathlib import Path


# --- 1. ì´ˆê¸° ì„¤ì • (í”„ë¡œê·¸ë¨ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰) ---

# ë¼ë²¨ ë§¤í•‘ ì •ì˜
ID2LABEL = {
    0: "Happiness",
    1: "Surprise",
    2: "Neutral",
    3: "Fear",
    4: "Disgust",
    5: "Anger",
    6: "Sadness"
}

# ì¥ì¹˜ ì„¤ì •
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")

# ëª¨ë¸ ë° Processor ì „ì—­ ë³€ìˆ˜ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°
try:
    print("ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œë¥¼ ë¡œë”©í•©ë‹ˆë‹¤...")
    SCRIPT_DIR = Path(__file__).resolve().parent
    # ëª¨ë¸ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ ìƒì„±
    MODEL_PATH = SCRIPT_DIR / "emotion_model.pt"
    
    MODEL = Wav2Vec2ForEmotion(num_labels=len(ID2LABEL))
    MODEL.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
    MODEL.to(DEVICE)
    MODEL.eval()  # ì¶”ë¡  ëª¨ë“œë¡œ ì„¤ì •

    PROCESSOR = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base")
    print("ë¡œë”© ì™„ë£Œ.")
except Exception as e:
    print(f"ì´ˆê¸°í™” ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
    MODEL = None
    PROCESSOR = None

# --- 2. ê°ì • ë¶„ì„ í•¨ìˆ˜ ---

def predict_emotion(voice_path: str) -> str:
    """
    ì£¼ì–´ì§„ ìŒì„± íŒŒì¼ ê²½ë¡œë¡œë¶€í„° ê°ì •ì„ ë¶„ì„í•˜ì—¬ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        voice_path (str): ë¶„ì„í•  .wav ìŒì„± íŒŒì¼ì˜ ê²½ë¡œ

    Returns:
        str: ë¶„ì„ëœ ê°ì • ë¬¸ìì—´ (ì˜ˆ: "Happiness"). ì—ëŸ¬ ë°œìƒ ì‹œ Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not MODEL or not PROCESSOR:
        print("ëª¨ë¸ ë˜ëŠ” í”„ë¡œì„¸ì„œê°€ ì œëŒ€ë¡œ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None

    if not os.path.exists(voice_path):
        print(f"ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - {voice_path}")
        return None

    try:
        # ğŸ”¹ ì˜¤ë””ì˜¤ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
        waveform, sr = torchaudio.load(voice_path)

        # ğŸ”¹ ìƒ˜í”Œë§ ë ˆì´íŠ¸ 16kHzë¡œ ë³€í™˜
        if sr != 16000:
            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)
            waveform = resampler(waveform)

        # ğŸ”¹ ì „ì²˜ë¦¬
        inputs = PROCESSOR(waveform.squeeze(), sampling_rate=16000, return_tensors="pt", padding=True)
        input_values = inputs["input_values"].to(DEVICE)
        attention_mask = inputs.get("attention_mask")
        if attention_mask is not None:
            attention_mask = attention_mask.to(DEVICE)
        
        # ğŸ”¹ ì¶”ë¡ 
        with torch.no_grad():
            outputs = MODEL(input_values=input_values, attention_mask=attention_mask)
            logits = outputs['logits']
            predicted_id = torch.argmax(logits, dim=-1).item()
        
        # ğŸ”¹ ê²°ê³¼ ë°˜í™˜
        return ID2LABEL[predicted_id]

    except Exception as e:
        print(f"ê°ì • ë¶„ì„ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return None


# --- 3. í•¨ìˆ˜ ì‚¬ìš© ì˜ˆì‹œ ---

if __name__ == "__main__":
    # ë¶„ì„í•  ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
    audio_file = "./input.wav"
    
    # í•¨ìˆ˜ í˜¸ì¶œ
    predicted_emotion = predict_emotion(audio_file)
    
    # ê²°ê³¼ ì¶œë ¥
    if predicted_emotion:
        print(f"\n[ìµœì¢… ë¶„ì„ ê²°ê³¼]")
        print(f"íŒŒì¼: {audio_file}")
        print(f"ê°ì •: {predicted_emotion}")
    else:
        print("\nê°ì • ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")