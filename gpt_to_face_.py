from __future__ import annotations
"""
livelink_voice_chat.py â€” Voiceâ€‘toâ€‘Voice GPT Demo (í™•ì¥íŒ)
-------------------------------------------------------
â€¢ ë§ˆì´í¬ ì…ë ¥, wav íŒŒì¼, **í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥** 3â€‘way ì¸í„°í˜ì´ìŠ¤
â€¢ í•œêµ­ì–´ ë‹µë³€ â†’ ë¡œë§ˆì ë³€í™˜(g2pk) â†’ ì˜ì–´ TTS â†’ NeuroSync ìŠ¤íŠ¸ë¦¬ë°
â€¢ OpenAI SDK â‰¥â€¯1.30 í˜¸í™˜ (audio ë„¤ì„ìŠ¤í˜ì´ìŠ¤)

â€» 2025â€‘06â€‘14 ì—…ë°ì´íŠ¸
    - voice_name ì„ utils/voice_conversion/pretrained/ ì•„ë˜ í´ë” ì¤‘ì—ì„œ ì„ íƒí•˜ë„ë¡ ë³€ê²½.
    - TTS ê²°ê³¼ë¥¼ numpy ë¡œ ë³€í™˜ í›„ RVC ì²˜ë¦¬, ë‹¤ì‹œ wav ë¡œ ì €ì¥í•˜ë„ë¡ í†µì¼.
    - build_voice_reply_* í•¨ìˆ˜ ì¤‘ë³µ í˜¸ì¶œ ë° ê²½ë¡œ/ë°ì´í„° íƒ€ì… í˜¼ë™ ìˆ˜ì •.
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° í‘œì¤€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os, sys, queue, tempfile, warnings
from pathlib import Path
from threading import Thread
from contextlib import suppress
from types import GeneratorType

import numpy as np
import soundfile as sf
import openai           # OpenAI Python SDK (>=1.30)
import pygame
import sounddevice as sd
from scipy.io.wavfile import write as wav_write

from g2pk import G2p    # ë¡œë§ˆì ë³€í™˜
from utils.voice_conversion.voice_conversion import run_voice_conversion

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í”„ë¡œì íŠ¸ ë‚´ë¶€ ëª¨ë“ˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from utils.files.file_utils import initialize_directories, ensure_wav_input_folder_exists, list_wav_files
from utils.romanize_ko import romanize_korean
from utils.audio_face_workers import process_wav_file
from livelink.connect.livelink_init import initialize_py_face, create_socket_connection
from livelink.animations.default_animation import default_animation_loop, stop_default_animation
from utils.emote_sender.send_emote import EmoteConnect

os.environ["OPENAI_API_KEY"] = ""
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¤ì •ê°’ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ENABLE_EMOTE_CALLS = False
TTS_MODEL        = "tts-1"
VOICE            = "nova"
TRANSCRIBE_MODEL = "whisper-1"
GPT_MODEL        = "gpt-4o-mini"
AUDIO_SAMPLE_RATE = 16_000

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìŒì„± ìºì‹œ ë””ë ‰í„°ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OUTPUT_WAV_DIR = Path.cwd() / "wav_cache"
OUTPUT_WAV_DIR.mkdir(parents=True, exist_ok=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ OpenAI í´ë¼ì´ì–¸íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
client = openai.OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    organization=os.getenv("OPENAI_ORGANIZATION"),
)

warnings.filterwarnings("ignore", message="Couldn't find ffmpeg or avconv")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Voice preset discovery â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
VOICE_PRETRAINED_DIR = Path(__file__).resolve().parent / "utils" / "voice_conversion" / "pretrained"
if not VOICE_PRETRAINED_DIR.exists():
    raise FileNotFoundError(f"Voice preset directory not found: {VOICE_PRETRAINED_DIR}")
AVAILABLE_VOICES = [d.name for d in VOICE_PRETRAINED_DIR.iterdir() if d.is_dir()]
if not AVAILABLE_VOICES:
    raise RuntimeError("No voice presets found in pretrained directory.")

def choose_voice() -> str:
    """ì‚¬ìš© ê°€ëŠ¥í•œ voice preset ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒ."""
    print("\nğŸ¤  ì‚¬ìš©í•  ë³´ì´ìŠ¤ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    for idx, name in enumerate(AVAILABLE_VOICES, 1):
        print(f"  {idx}) {name}")
    while True:
        sel = input("> ").strip()
        if sel.isdigit() and 1 <= int(sel) <= len(AVAILABLE_VOICES):
            return AVAILABLE_VOICES[int(sel) - 1]
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ë²ˆí˜¸ë¥¼ ë‹¤ì‹œ ì…ë ¥í•˜ì„¸ìš”.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì˜¤ë””ì˜¤ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def record_microphone() -> Path:
    """ì—”í„° â†’ ë…¹ìŒ ì‹œì‘, ì—”í„° â†’ ì¢…ë£Œ í›„ temp wav íŒŒì¼ ë°˜í™˜"""
    q: queue.Queue[np.ndarray] = queue.Queue()

    def _callback(indata, _frames, _time, status):
        if status:
            print(status, file=sys.stderr)
        q.put(indata.copy())

    print("\nğŸ™ï¸  Enter â†’ ì‹œì‘, Enter â†’ ì¢…ë£Œ"); input(); print("Recordingâ€¦")
    with sd.InputStream(samplerate=AUDIO_SAMPLE_RATE, channels=1, dtype="int16", callback=_callback):
        input()
    print("Recording stopped. Processingâ€¦")

    if q.empty():
        raise RuntimeError("No audio captured.")
    audio_np = np.concatenate([q.get() for _ in range(q.qsize())], axis=0)
    fd, path = tempfile.mkstemp(suffix=".wav"); os.close(fd)
    wav_write(path, AUDIO_SAMPLE_RATE, audio_np)
    return Path(path)


def transcribe_audio(wav_path: Path) -> str:
    with wav_path.open("rb") as f:
        return client.audio.transcriptions.create(model=TRANSCRIBE_MODEL, file=f, response_format="text").strip()


def gpt_response(prompt: str, history: list[dict]) -> str:
    history.append({"role": "user", "content": prompt})
    resp = client.chat.completions.create(model=GPT_MODEL, messages=history, temperature=0.7)
    answer = resp.choices[0].message.content.strip()
    history.append({"role": "assistant", "content": answer})
    return answer


def text_to_speech_(text: str, speed: float = 1.0) -> Path:
    resp = client.audio.speech.create(model=TTS_MODEL, voice=VOICE, input=text, response_format="wav", speed=speed)
    fd, path = tempfile.mkstemp(suffix=".wav"); os.close(fd)
    resp.stream_to_file(path)
    return Path(path)

# â€•â€• TTS â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•
TTS_MODEL   = "tts-1"
TTS_VOICE   = "nova"     # OpenAI preset voice (ì¤‘ë¦½ì , SNR ë†’ìŒ)
TTS_SPEED   = 0.8         # 0.8Â ~Â 1.0 ê¶Œì¥ ë²”ìœ„ â€” ë³´í†µ ì†ë„
# -------------------------------------------------------------
def text_to_speech(text: str, speed: float = 1.0) -> Path:
    """í…ìŠ¤íŠ¸ â†’ wav (nova, speed 0.9)"""
    resp = client.audio.speech.create(
        model=TTS_MODEL,
        voice=TTS_VOICE,
        input=text,
        response_format="wav",
        speed=TTS_SPEED,
    )
    fd, path = tempfile.mkstemp(suffix=".wav"); os.close(fd)
    resp.stream_to_file(path)
    return Path(path)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í—¬í¼: numpy â†’ temp wav â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def numpy_to_temp_wav(audio_np: np.ndarray, sr: int) -> Path:
    fd, path = tempfile.mkstemp(suffix=".wav"); os.close(fd)
    sf.write(path, audio_np, sr)
    return Path(path)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í—¬í¼: numpy â†’ ì§€ì • ê²½ë¡œ wav â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def numpy_to_wav_in_cache(audio_np: np.ndarray, sr: int) -> Path:
    """wav_cache/voice_name_<timestamp>.wav ë¡œ ì €ì¥ í›„ Path ë°˜í™˜"""
    
    file_name = f"rvc.wav"
    path = OUTPUT_WAV_DIR / file_name
    sf.write(path, audio_np, sr)
    return path

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ V2V íŒŒì´í”„ë¼ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _rvc_pipeline(tts_path: Path, voice_name: str) -> Path:
    """TTS wav â†’ numpy â†’ RVC â†’ temp wav Path ë°˜í™˜"""
    wav_np, sr = sf.read(tts_path, dtype="float32")
    converted_np, converted_sr = run_voice_conversion(
        src_audio=wav_np,
        src_sr=sr,
        transpose=0,
        voice_name=voice_name,
    )
    return numpy_to_wav_in_cache(converted_np, converted_sr)


def build_voice_reply_audio(wav_path: Path, history: list[dict], voice_name: str) -> Path:
    """ìŒì„± ì…ë ¥ â†’ STT â†’ GPT â†’ ë¡œë§ˆì â†’ TTS â†’ RVC"""
    user_text    = transcribe_audio(wav_path)
    assistant_ko = gpt_response(user_text, history)
    assistant_en = romanize_korean(assistant_ko)
    tts_path     = text_to_speech(assistant_en, speed=0.9)
    return _rvc_pipeline(tts_path, voice_name)


def build_voice_reply_text(user_text: str, history: list[dict], voice_name: str) -> Path:
    """í…ìŠ¤íŠ¸ ì…ë ¥ â†’ GPT â†’ ë¡œë§ˆì â†’ TTS â†’ RVC"""
    assistant_ko = gpt_response(user_text, history)
    assistant_en = romanize_korean(assistant_ko)
    tts_path     = text_to_speech(assistant_en, speed=0.9)
    
    return _rvc_pipeline(tts_path, voice_name)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì…ë ¥ ì†ŒìŠ¤ ì œë„ˆë ˆì´í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def iterate_audio_inputs(folder: Path):
    while True:
        files = list_wav_files(str(folder))
        if not files:
            print("Put .wav files in 'wav_input' or use --mic."); break
        for idx, f in enumerate(files, 1):
            print(f"{idx}: {f}")
        sel = input("Select number (q to quit): ").strip()
        if sel.lower() in {"q", "quit", "exit"}: break
        if sel.isdigit() and 1 <= int(sel) <= len(files):
            yield Path(folder / files[int(sel)-1])
        else:
            print("Invalid selection.")

def audio_source_iter(wav_folder: Path):
    while True:
        choice = input("\nâ–¶ ì…ë ¥ ëª¨ë“œ\n  1) ğŸ™ï¸  ë§ˆì´í¬\n  2) ğŸ“‚ wav íŒŒì¼\n  3) âœï¸  í…ìŠ¤íŠ¸\n  q) ì¢…ë£Œ\n> ").strip().lower()
        if choice == "1":
            try:
                yield ("audio", record_microphone())
            except KeyboardInterrupt:
                print("â¹ï¸  ì·¨ì†Œ")
        elif choice == "2":
            for p in iterate_audio_inputs(wav_folder):
                yield ("audio", p)
        elif choice == "3":
            txt = input("ğŸ“  í…ìŠ¤íŠ¸ ì…ë ¥: \n> ").strip()
            if txt:
                yield ("text", txt)
        elif choice in {"q", "quit", "exit"}: break
        else:
            print("âŒ ì˜ëª»ëœ ì…ë ¥")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main() â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    initialize_directories()
    wav_dir = Path.cwd() / "wav_input"
    ensure_wav_input_folder_exists(str(wav_dir))

    py_face     = initialize_py_face()
    socket_conn = create_socket_connection()

    anim_th = Thread(target=default_animation_loop, args=(py_face,), daemon=True)
    anim_th.start()

    voice_name = choose_voice()
    print(f"\nâœ… ì„ íƒëœ ë³´ì´ìŠ¤: {voice_name}\n")

    history = [{
        "role": "system",
        "content": (
            "You are a helpful assistant. "
            "ë‹µë³€ì€ **í•œê¸€**ë¡œ, ìµœëŒ€ 100ìë¡œ ì œí•œí•´."
        ),
    }]

    for input_type, payload in audio_source_iter(wav_dir):
        if ENABLE_EMOTE_CALLS:
            EmoteConnect.send_emote("startspeaking")
        try:
            if input_type == "audio":
                assistant_audio = build_voice_reply_audio(payload, history, voice_name)
            else:
                assistant_audio = build_voice_reply_text(payload, history, voice_name)
            print("asdasdasd", type(assistant_audio),type(py_face), type(socket_conn), type(anim_th))
            process_wav_file(assistant_audio, py_face, socket_conn, anim_th)
        finally:
            if ENABLE_EMOTE_CALLS:
                EmoteConnect.send_emote("stopspeaking")
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬ (assistant_audioëŠ” í•­ìƒ Path)
            for p in [assistant_audio] if input_type == "text" else [assistant_audio, payload]:
                if isinstance(p, Path) and p.exists() and p.parent == Path(tempfile.gettempdir()):
                    with suppress(PermissionError):
                        p.unlink(missing_ok=True)

    stop_default_animation.set(); anim_th.join(timeout=2)
    pygame.quit(); socket_conn.close()




if __name__ == "__main__":
    main()
